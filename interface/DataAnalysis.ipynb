{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Imports '''\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from dbfread import DBF\n",
    "from pyproj import CRS\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Paths '''\n",
    "\n",
    "algeria_path = 'Algeria/algeria'\n",
    "soil_path = 'data/soil_dz_allprops.csv'\n",
    "\n",
    "soil_df = pd.read_csv(soil_path)\n",
    "# combined_climate_seasons_df = pd.read_csv('Climate_data/Combined_Seasonal_Climate_Data.csv')\n",
    "Combined_Climate_Seasonal_columns_df = pd.read_csv('data/Combined_Climate_Seasonal_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_Climate_Seasonal_columns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Input : - the name of the data exp : Soil, Climate, Shape...\n",
    "            - in the case of the climate dataset : provide the path  \n",
    "\n",
    "    Output : a dataframe\n",
    "'''\n",
    "\n",
    "def load_dataset (dataset_name, climat_path = None):\n",
    "    if (dataset_name == \"Soil\"):\n",
    "        return pd.read_csv(soil_path)\n",
    "\n",
    "    elif (dataset_name == \"Climat\"):\n",
    "        return pd.read_csv(climat_path)\n",
    "\n",
    "    elif (dataset_name == \"Shape\"):\n",
    "        return gpd.read_file(f'{algeria_path}.shp', encoding='utf-8')\n",
    "\n",
    "    elif (dataset_name == \"Index\"):\n",
    "        return gpd.read_file(f'{algeria_path}.shx', encoding='utf-8')\n",
    "\n",
    "    elif (dataset_name == \"DBF\"):\n",
    "        table = DBF(f'{algeria_path}.dbf', encoding='utf-8') \n",
    "        return pd.DataFrame(iter(table))\n",
    "\n",
    "    elif (dataset_name == \"PRJ\"):\n",
    "        with open(f'{algeria_path}.prj', 'r') as f:\n",
    "            prj_text = f.read()\n",
    "        return CRS.from_wkt(prj_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values and Unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to calculate the amount of missing values and unique values\n",
    "    Input : A dataframe\n",
    "    Output : A dataframe with Three columns\n",
    "            - First one : Name of the column\n",
    "            - Second one : The  number of missing values\n",
    "            - Third one : The number of unique values\n",
    "\n",
    "'''\n",
    "def calculate_missing_and_unique(data):\n",
    "    missing_values = data.isnull().sum()\n",
    "    unique_values = data.nunique()\n",
    "    return pd.DataFrame({\"missing_values\": missing_values, \"unique_values\": unique_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_missing_and_unique(soil_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Central Tendencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**skewness** : tells how much and in which direction the data deviates from a normal (symmetrical) distribution\n",
    "* Positive Skewness (Right-skewed): The data's tail is longer on the right side. The mean > the median.\n",
    "* Negative Skewness (Left-skewed): The data's tail is longer on the left side. The mean < the median.\n",
    "* Skewness of 0: The data is perfectly symmetrical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function that calculates the central tendencies of an attribute.\n",
    "        Input : A dataframe and the name of the column\n",
    "        Ouput : A dict with the value of :\n",
    "                - The mean\n",
    "                - The median\n",
    "                - The mode\n",
    "                - The symmetry\n",
    "                - The skewness \n",
    "'''\n",
    "def calculate_central_tendency(data, column):\n",
    "    mean = data[column].mean()\n",
    "    median = data[column].median()\n",
    "    mode = data[column].mode()\n",
    "\n",
    "    if len(mode) > 1:  # Check if there are multiple modes\n",
    "        symmetry = \"Skewed (multimodal)\" \n",
    "    else:\n",
    "        mode_value = mode.iloc[0]  # Use the first mode value\n",
    "        symmetry = \"Symmetric\" if np.isclose(mean, median) and np.isclose(mean, mode_value) else \"Skewed\" #(skewed == asymétrique)\n",
    "\n",
    "    skewness = skew(data[column].dropna())  # Drop NaNs for skewness calculation \n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"median\": median,\n",
    "        \"mode\": mode.to_list(),\n",
    "        \"symmetry\": symmetry,\n",
    "        \"skewness\": skewness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_central_tendency(Combined_Climate_Seasonal_columns_df, 'Summer_PSurf_max')\n",
    "\n",
    "# calculate_central_tendency(soil_df, 'clay % subsoil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    To display the central tendencies of all the columns of the soil dataset and ignore the geometry column\n",
    "'''\n",
    "for column in soil_df.columns:\n",
    "    try:\n",
    "        print(column, \" : \", calculate_central_tendency(soil_df, column))\n",
    "    except TypeError as e:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures of dispersion + outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standard deviation (l’écart-type) :\n",
    "    * Quantifies how much the values deviate from the mean of the dataset.\n",
    "    * A low standard deviation : means that the values are close to the mean, indicating low variability in the data.\n",
    "    * A high standard deviation : means that the values are spread out over a wider range, indicating high variability.\n",
    "\n",
    "* Variance : \n",
    "    * Gives a measure of the spread of data points in squared units (if the data is in meters, the variance will be in square meters).\n",
    "    * Tells how far each data point is from the mean, on average.\n",
    "\n",
    "* Both **std_dev** and **variance** are measures of the variability or spread of a dataset, but **std_dev** is more commonly used because it is in the original units of the data, making it easier to interpret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to calculat the dispersion of the data\n",
    "    Input : A dataframe + The name of the attribute\n",
    "    Output : A dict with :\n",
    "            - The standard deviation\n",
    "            - The variance\n",
    "            - The min value\n",
    "            - The median\n",
    "            - The max value\n",
    "            - The IQR \n",
    "            - A list of outliers (Show the full row and its index in the dataframe)\n",
    "'''\n",
    "def calculate_dispersion(data, column):\n",
    "\n",
    "    std_dev = data[column].std() \n",
    "    variance = data[column].var()\n",
    "    \n",
    "    Q0 = data[column].quantile(0)\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q2 = data[column].quantile(0.50)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    Q4 = data[column].quantile(1)\n",
    "\n",
    "    iqr = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * iqr\n",
    "    upper_bound = Q3 + 1.5 * iqr\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "\n",
    "    return {\n",
    "        \"std_dev\": std_dev,\n",
    "        \"variance\": variance,\n",
    "        \"min\": Q0,\n",
    "        \"median\": Q2,\n",
    "        \"max\": Q4,\n",
    "        \"iqr\": iqr,\n",
    "        \"outliers\": outliers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion = calculate_dispersion(Combined_Climate_Seasonal_columns_df, 'Fall_PSurf_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion['outliers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BoxPlot with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to construct boxplots and display outliers\n",
    "    Input : - A dataframe\n",
    "            - The attribute name\n",
    "            - A bool value to Show outliers or not\n",
    "    Output : A boxplot      \n",
    " '''\n",
    "def plot_boxplot(data, columns, show_outliers=True):\n",
    "    \n",
    "    plt.boxplot([data[column] for column in columns],\n",
    "                vert=True, #Orientation of the box plot = vertical (False is horizental)\n",
    "                patch_artist=True, # Enables filling the box with color.\n",
    "                showfliers=show_outliers,\n",
    "                boxprops=dict(facecolor='lightblue', color='black'), \n",
    "                medianprops=dict(color='red'), \n",
    "                whiskerprops=dict(color='black'),\n",
    "                capprops=dict(color='orange'))\n",
    "    \n",
    "    plt.title(\"Box Plots\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.xticks(range(1, len(columns) + 1), columns, rotation=45, ha='right')  # Set x-ticks to column names\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(soil_df, ['sand % topsoil', 'sand % subsoil', 'silt % topsoil', 'silt% subsoil', 'clay % subsoil'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histograms and data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to construct histograms and visualize the data distribution\n",
    "    Input : A dataframe + The attribute name\n",
    "    Ouput : A histogramme plot     \n",
    "'''\n",
    "\n",
    "def plot_histogram(data, column):\n",
    "    \n",
    "    sns.histplot(data[column].dropna(), kde=True)\n",
    "    plt.title(f\"Histogram of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in soil_df.columns:\n",
    "    if column == 'geometry':\n",
    "        continue  # Skip this column\n",
    "    plot_histogram(soil_df, column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(df, 'sand % topsoil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatter plots and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to construct scatter plots\n",
    "    Input : - A dataframe\n",
    "            - The first attribute name\n",
    "            - The second attribute name\n",
    "            \n",
    "    Ouput : - A scatter plot\n",
    "            - The % of correlation\n",
    "'''\n",
    "def plot_scatter(data, column1, column2):\n",
    "\n",
    "    sns.scatterplot(data=data, x=column1, y=column2)\n",
    "\n",
    "    plt.title(f\"Scatter plot of {column1} vs {column2}\")\n",
    "    plt.xlabel(column1)\n",
    "    plt.ylabel(column2)\n",
    "    \n",
    "    correlation = data[[column1, column2]].corr().iloc[0, 1]\n",
    "    print(f\"Correlation between {column1} and {column2}: {correlation}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(soil_df, 'sand % topsoil', 'sand % subsoil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df, 'silt % topsoil', 'silt% subsoil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Funtion to plot the soil map \n",
    "    Input : A dataframe + The name of the attribute\n",
    "    Output : A map plot \n",
    "'''\n",
    "def plot_soil_map(soil_df, attribute):\n",
    "\n",
    "    # Convertir en GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(soil_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "    # Tracer la carte avec le colormap \"viridis\" pour correspondre aux couleurs de l'image\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    # cmap='YlOrBr'\n",
    "    gdf.plot(column=attribute, cmap='viridis', legend=True, \n",
    "            legend_kwds={'label': attribute, 'orientation': \"horizontal\"},\n",
    "            ax=ax)\n",
    "\n",
    "    # Personnaliser le graphique\n",
    "    plt.title(f\"{attribute} across Algeria\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "plot_soil_map(soil_df, 'sand % topsoil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Funtion to plot the climate map \n",
    "    Input : A dataframe + The name of the attribute\n",
    "    Output : A map plot \n",
    "'''\n",
    "def plot_climate_map(Combined_Climate_Seasonal_columns_df, attribute):\n",
    "\n",
    "    df_data = Combined_Climate_Seasonal_columns_df[['lon', 'lat', attribute]]\n",
    "    pivoted_data = df_data.pivot_table(index=\"lat\", columns=\"lon\", values=attribute, aggfunc=\"mean\")\n",
    "\n",
    "    # Define the geographic extent of the heatmap (min and max longitude and latitude)\n",
    "    extent = [df_data['lon'].min(), df_data['lon'].max(), df_data['lat'].min(), df_data['lat'].max()]\n",
    "\n",
    "    # Plot the heatmap using imshow with the defined extent\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(pivoted_data, cmap=\"coolwarm\", extent=extent, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(im, label=attribute)\n",
    "\n",
    "    gdf_algerie = gpd.read_file(f'{algeria_path}.shp', encoding=\"utf-8\")\n",
    "    gdf_algerie.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title(f\"{attribute} Heatmap\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_climate_map(Combined_Climate_Seasonal_columns_df, 'Spring_PSurf_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Funtion to plot a country's map\n",
    "    Input : The path to the shape file\n",
    "    Output : A map plot \n",
    "'''\n",
    "def plot_algeria_map(path):\n",
    "    # Load the shapefile\n",
    "    gdf_shp = gpd.read_file(f'{path}', encoding='utf-8')\n",
    "    country_name = path.split(\"/\")[0].split(\".\")[0]\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    gdf_shp.plot(ax=ax, color=\"lightgreen\", edgecolor=\"black\")\n",
    "\n",
    "    # Add title and labels\n",
    "    ax.set_title(f\"Map of {country_name}\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_algeria_map(f'{algeria_path}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reduction through Aggregation by Seasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to get the data's season based on its time \n",
    "    Input : A dataframe' time column\n",
    "    Output : The season's name (Fall, Winter, Spring, Summer)\n",
    "'''\n",
    "def get_season(date):\n",
    "    if date >= pd.Timestamp(year=date.year, month=12, day=22) or date < pd.Timestamp(year=date.year, month=3, day=22):\n",
    "        return 'Winter'\n",
    "    elif date >= pd.Timestamp(year=date.year, month=3, day=22) and date < pd.Timestamp(year=date.year, month=6, day=22):\n",
    "        return 'Spring'\n",
    "    elif date >= pd.Timestamp(year=date.year, month=6, day=22) and date < pd.Timestamp(year=date.year, month=9, day=22):\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to aggregate data by season\n",
    "    Input : - The file pattern (In this case we have 12 files (One for each month) for each type of climate )\n",
    "            - The climate attribute name (Exp : PSurf or Qair or Wind ...)\n",
    "\n",
    "    Output : A dataframe of that specific climate attribute with :\n",
    "            - Dropped time column\n",
    "            - all the data grouped by longitude - latitude - season\n",
    "            - Only the (Min - Max - Mean) values\n",
    "'''\n",
    "def seasonal_aggregation(file_pattern, variable_name):\n",
    "    # Load all monthly files into a single DataFrame\n",
    "    all_months = pd.concat([pd.read_csv(f) for f in glob.glob(file_pattern)], ignore_index=True)\n",
    "    \n",
    "    # Convert 'time' to datetime (originally 'object' converted to 'datetime64[ns]')\n",
    "    all_months['time'] = pd.to_datetime(all_months['time'])\n",
    "    \n",
    "    # Assign season name based on date\n",
    "    all_months['season'] = all_months['time'].apply(get_season)\n",
    "    \n",
    "    # Group by latitude, longitude, season, and calculate min, max, mean\n",
    "    seasonal_data = all_months.groupby(['lat', 'lon', 'season']).agg(\n",
    "        min_value=(variable_name, 'min'),\n",
    "        max_value=(variable_name, 'max'),\n",
    "        mean_value=(variable_name, 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return seasonal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to CSV files for each dataset (files patterns)\n",
    "dataset_info = {\n",
    "    'PSurf': 'Climate_data/csv_filtered_climat_data/PSurf_WFDE5_CRU_2019*_v2.csv',\n",
    "    'Qair': 'Climate_data/csv_filtered_climat_data/Qair_WFDE5_CRU_2019*_v2.csv',\n",
    "    'Rainf': 'Climate_data/csv_filtered_climat_data/Rainf_WFDE5_CRU_2019*_v2.csv',\n",
    "    'Snowf': 'Climate_data/csv_filtered_climat_data/Snowf_WFDE5_CRU_2019*_v2.csv',\n",
    "    'Tair': 'Climate_data/csv_filtered_climat_data/Tair_WFDE5_CRU_2019*_v2.csv',\n",
    "    'Wind': 'Climate_data/csv_filtered_climat_data/Wind_WFDE5_CRU_2019*_v2.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform aggregation for each dataset and save it as a csv file\n",
    "seasonal_aggregates = {}\n",
    "for variable, path in dataset_info.items():\n",
    "    seasonal_aggregates[variable] = seasonal_aggregation(path, variable)\n",
    "    # seasonal_aggregates[variable].to_csv(f'Climate_data/Seasonal_filtered_climate_data/{variable}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Integration: Merges Data from Multiple Sources into a Single Coherent Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge Seasonal Climate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to :\n",
    "* Put all the seasons in one dataset, keeping only the (min - max - mean) values \n",
    "* turn the seasons into columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function to merge the seasonal datasets into one single dataset, and keeping only min - max - mean values\n",
    "\n",
    "    input : path to the files = Climate_data/Seasonal_filtered_climate_data/*.csv\n",
    "    output : saves the dataset into a csv file\n",
    "'''\n",
    "def merge_climate_datasets():\n",
    "    # Path to the saved seasonal CSV files\n",
    "    file_paths = glob.glob('Climate_data/Seasonal_filtered_climate_data/*.csv')\n",
    "\n",
    "    # Initialize an empty list to store each dataset\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each file and add it to the list of dataframes\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        variable_name = file_path.split(\"\\\\\")[-1].split('.')[0] # Extract variable name from filename\n",
    "        df = df.rename(columns={'min_value': f'{variable_name}_min',\n",
    "                                'max_value': f'{variable_name}_max',\n",
    "                                'mean_value': f'{variable_name}_mean'})\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Merge all dataframes on common columns\n",
    "    merged_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=['lat', 'lon', 'season'], how='outer')\n",
    "\n",
    "    # Save the merged dataset to a new CSV file\n",
    "    # merged_df.to_csv('Climate_data/Combined_Seasonal_Climate_Data.csv', index=False)\n",
    "\n",
    "# merge_climate_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function to pivot the dataset to make seasons as columns and save the new dataset into a csv file\n",
    "'''\n",
    "def pivot_climate_seasons():\n",
    "    combined_seasoned_climat_df = pd.read_csv('Climate_data/Combined_Seasonal_Climate_Data.csv')\n",
    "    pivoted_df = combined_seasoned_climat_df.pivot_table(\n",
    "        index=['lat', 'lon'],\n",
    "        columns='season',\n",
    "        values=[col for col in combined_seasoned_climat_df.columns if col not in ['lat', 'lon', 'season']]\n",
    "    )\n",
    "\n",
    "    # Flatten the multi-index columns\n",
    "    pivoted_df.columns = [f\"{season}_{metric}\" for metric, season in pivoted_df.columns]\n",
    "\n",
    "    # Reset the index if needed\n",
    "    pivoted_df = pivoted_df.reset_index()\n",
    "\n",
    "    # Save the transformed dataframe\n",
    "    pivoted_df.to_csv('Combined_Climate_Seasonal_columns.csv', index=False)\n",
    "\n",
    "# pivot_climate_seasons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge Climate dataset with soil dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function to Merge the climate dataset with the soil dataset based on spatial relationships.\n",
    "    \n",
    "    Input : The climate datafram + The soil dataframe\n",
    "    Output : One single dataframe with concatenated data\n",
    "'''\n",
    "def merge_climate_soil(climate_df, soil_gdf):\n",
    "    # Ensure the soil dataset is a GeoDataFrame\n",
    "    soil_gdf['geometry'] = soil_gdf['geometry'].apply(wkt.loads)\n",
    "    soil_gdf = gpd.GeoDataFrame(soil_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "    \n",
    "    # Create GeoDataFrame for the climate dataset\n",
    "    climate_gdf = gpd.GeoDataFrame(\n",
    "        climate_df,\n",
    "        geometry=[Point(lon, lat) for lon, lat in zip(climate_df['lon'], climate_df['lat'])],\n",
    "        crs=\"EPSG:4326\"  # Assuming WGS84 coordinate system\n",
    "    )\n",
    "    \n",
    "    # Perform spatial join\n",
    "    merged_gdf = gpd.sjoin(climate_gdf, soil_gdf, how='inner', predicate='within')\n",
    "    \n",
    "    return merged_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_climate_soil_df = merge_climate_soil(Combined_Climate_Seasonal_columns_df, soil_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_climate_soil_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Choices of Handling Outliers and Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to either delete the missing data or replace it by the mean or the median\n",
    "    input : - A dataframe\n",
    "            - The attribute name\n",
    "            - The method (remove / mean / median)\n",
    "\n",
    "    Output : Dataframe without missing values  \n",
    "'''\n",
    "\n",
    "def handle_missing_data(data, column, method='remove'):\n",
    "    # Handle missing values\n",
    "    if method == 'remove':\n",
    "        data = data.dropna(subset=[column])\n",
    "    elif method == 'mean':\n",
    "        data[column] = data[column].fillna(data[column].mean())\n",
    "    elif method == 'median':\n",
    "        data[column] = data[column].fillna(data[column].median())\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to handle outliers\n",
    "    input : - A dataframe\n",
    "            - The attribute name\n",
    "            - The method (IQR)\n",
    "\n",
    "    Output : Dataframe without outliers\n",
    "'''\n",
    "\n",
    "def handle_outliers(data, column, outlier_method='IQR'):\n",
    "\n",
    "    # Handle outliers (using IQR method)\n",
    "    if outlier_method == 'IQR':\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        iqr = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * iqr\n",
    "        upper_bound = Q3 + 1.5 * iqr\n",
    "        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "combined_seasoned_climat_df = pd.read_csv('Combined_Climate_Seasonal_columns.csv')\n",
    "for column in combined_seasoned_climat_df.columns:\n",
    "    if column in ['lat', 'lon', 'season']:\n",
    "        continue  # Skip this column\n",
    "    combined_seasoned_climat_df = handle_missing_data(combined_seasoned_climat_df, column)\n",
    "    combined_seasoned_climat_df = handle_outliers(combined_seasoned_climat_df, column, outlier_method='IQR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(combined_seasoned_climat_df, ['Fall_PSurf_min', 'Fall_PSurf_max', 'Fall_PSurf_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalization: Min-Max / Z-Score Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to normalize the data with one of 2 methodes (Z-score or Min-Max)\n",
    "    Input : - A Dataframe\n",
    "            - The attribute name\n",
    "            - the normalization method name\n",
    "    Output : The dataframe with normalized data\n",
    "'''\n",
    "\n",
    "def normalize_data(data, column, method='z-score'):\n",
    "    if method == 'z-score':\n",
    "        if (data[column].std()) == 0: \n",
    "            data[column] = 0\n",
    "        else: \n",
    "            data[column] = (data[column] - data[column].mean()) / data[column].std()\n",
    "    elif method == 'min-max':\n",
    "        scaler = MinMaxScaler()\n",
    "        data[column] = scaler.fit_transform(data[[column]])\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for all the columns of the climate dataset\n",
    "Combined_Climate_Seasonal_columns_df = pd.read_csv('Combined_Climate_Seasonal_columns.csv')\n",
    "for column in Combined_Climate_Seasonal_columns_df.columns:\n",
    "    if column in ['lat', 'lon', 'season']:\n",
    "        continue  # Skip this column\n",
    "    d = normalize_data(Combined_Climate_Seasonal_columns_df, column, method='min-max')\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reduction via Discretization of Continuous Data: Equal Frequency / Amplitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to discretize data with one of 2 methodes (equal-amplitude or equal-frequency)\n",
    "     Input : - A Dataframe\n",
    "            - The attribute name\n",
    "            - the discretization method name\n",
    "    Output : The dataframe with discretized data\n",
    "'''\n",
    "def discretize_data(data, column, method='equal-frequency'):\n",
    "    n = len(data[column])\n",
    "    K = int(1 + (10 / 3) * np.log10(n))\n",
    "    print(f\"The number of intervals = {K}\\n\")\n",
    "\n",
    "    if method == 'equal-amplitude':\n",
    "        # Equal-amplitude (equal-width) binning\n",
    "        min_value = data[column].min()\n",
    "        max_value = data[column].max()\n",
    "        width = (max_value - min_value) / K\n",
    "        bins = [min_value + i * width for i in range(K + 1)]\n",
    "        print(f\"The interval's bins (equal-amplitude) are: {bins}\\n\")\n",
    "\n",
    "        # Calculate labels as mean values for each bin\n",
    "        labels = []\n",
    "        for i in range(K):\n",
    "            bin_data = data[(data[column] >= bins[i]) & (data[column] < bins[i + 1])]\n",
    "            mean_value = bin_data[column].mean() if not bin_data.empty else (bins[i] + bins[i + 1]) / 2\n",
    "            labels.append(mean_value)\n",
    "        print(f\"The interval's labels (equal-amplitude) are: {labels}\\n\")\n",
    "\n",
    "        d = pd.cut(data[column], bins=bins, labels=labels)\n",
    "        data[column] = d\n",
    "        \n",
    "    elif method == 'equal-frequency':\n",
    "        # Equal-frequency binning\n",
    "        d = pd.qcut(data[column], q=K, labels=False, duplicates='drop')\n",
    "        \n",
    "        # Calculate mean for each equal-frequency bin\n",
    "        labels = []\n",
    "        for bin_number in range(K):\n",
    "            bin_data = data[d == bin_number]\n",
    "            mean_value = bin_data[column].mean()\n",
    "            labels.append(mean_value)\n",
    "        print(f\"The interval's labels (equal-frequency) are: {labels}\\n\")\n",
    "        \n",
    "        # Reassign the discretized column with mean labels\n",
    "        d = d.map(lambda x: labels[int(x)] if pd.notna(x) else None)\n",
    "\n",
    "    data[column] = d\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the climate dataset\n",
    "Combined_Climate_Seasonal_columns_df = pd.read_csv('Combined_Climate_Seasonal_columns.csv')\n",
    "d = discretize_data(Combined_Climate_Seasonal_columns_df, 'Fall_PSurf_max', method='equal-amplitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reduction (Elimination of Redundancies) Horizontal / Vertical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Function to eliminate redundancies horizontaly or vertically\n",
    "    Input : - A dataframe\n",
    "            - The reduction methode\n",
    "            - The columns names in the case of the vertical reduction\n",
    "\n",
    "    Output : Dataframe without redundancies\n",
    "'''\n",
    "def eliminate_redundancies(data, method='vertical', columns=None):\n",
    "    if method == 'horizontal':\n",
    "        data = data.drop_duplicates()\n",
    "    elif method == 'vertical' and columns:\n",
    "        data = data[columns]  # Keep only the necessary columns\n",
    "    return data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
